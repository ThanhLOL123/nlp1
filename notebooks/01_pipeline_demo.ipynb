{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.pipeline import VietnameseNLPPipeline\n",
    "from src.data_loader import VietnameseDatasetLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = VietnameseNLPPipeline('../configs/config.yaml')\n",
    "data_loader = VietnameseDatasetLoader('../data')\n",
    "\n",
    "print(\"Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test simple sentences\n",
    "test_sentences = [\n",
    "    \"Xin chào Việt Nam\",\n",
    "    \"Tôi đang học xử lý ngôn ngữ tự nhiên\",\n",
    "    \"Trường Đại học Bách khoa Hà Nội\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    result = pipeline.process(sentence, verbose=True)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import VietnameseTokenizer\n",
    "\n",
    "text = \"Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò\"\n",
    "\n",
    "# Compare different tokenizers\n",
    "tokenizers = {\n",
    "    'underthesea': VietnameseTokenizer('underthesea'),\n",
    "    'pyvi': VietnameseTokenizer('pyvi'),\n",
    "    'hybrid': VietnameseTokenizer('hybrid')\n",
    "}\n",
    "\n",
    "print(f\"Original text: {text}\\n\")\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"{name:12}: {' | '.join(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student feedback dataset\n",
    "student_feedback = data_loader.load_student_feedback()\n",
    "test_df = student_feedback['test'].head(10)\n",
    "\n",
    "# Process and analyze POS tags\n",
    "pos_distribution = {}\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    result = pipeline.process(row['sentence'])\n",
    "    for token, tag in result.pos_tags:\n",
    "        if tag not in pos_distribution:\n",
    "            pos_distribution[tag] = 0\n",
    "        pos_distribution[tag] += 1\n",
    "\n",
    "# Visualize POS distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "tags = list(pos_distribution.keys())\n",
    "counts = list(pos_distribution.values())\n",
    "\n",
    "plt.bar(tags, counts)\n",
    "plt.xlabel('POS Tags')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('POS Tag Distribution in Student Feedback Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tone_processor import ToneProcessor\n",
    "\n",
    "tone_processor = ToneProcessor()\n",
    "\n",
    "# Test tone removal and addition\n",
    "test_cases = [\n",
    "    \"Việt Nam tươi đẹp\",\n",
    "    \"Hôm nay trời mưa rất to\",\n",
    "    \"Cảm ơn bạn đã giúp đỡ\"\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    \n",
    "    # Remove tones\n",
    "    no_tones = tone_processor.remove_tones(text)\n",
    "    print(f\"No tones: {no_tones}\")\n",
    "    \n",
    "    # Add tones back\n",
    "    restored = tone_processor.add_tones(no_tones)\n",
    "    print(f\"Restored: {restored}\")\n",
    "    \n",
    "    # Analyze tones\n",
    "    analysis = tone_processor.analyze_text(text)\n",
    "    print(f\"Tone distribution: {analysis['tone_distribution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common errors in tokenization\n",
    "error_examples = [\n",
    "    (\"Ông Nguyễn Văn A\", [\"Ông\", \"Nguyễn_Văn_A\"]),  # Name handling\n",
    "    (\"21/12/2024\", [\"21/12/2024\"]),  # Date handling\n",
    "    (\"email@example.com\", [\"email@example.com\"]),  # Email handling\n",
    "    (\"Tp.HCM\", [\"Tp.\", \"HCM\"]),  # Abbreviation\n",
    "]\n",
    "\n",
    "print(\"Common Tokenization Challenges:\\n\")\n",
    "for text, expected in error_examples:\n",
    "    result = pipeline.process(text)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Got: {result.tokens}\")\n",
    "    print(f\"Match: {result.tokens == expected}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure processing speed\n",
    "texts = test_df['sentence'].tolist()[:100]\n",
    "\n",
    "start_time = time.time()\n",
    "results = pipeline.process_batch(texts)\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time = total_time / len(texts)\n",
    "\n",
    "print(f\"Processed {len(texts)} sentences\")\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per sentence: {avg_time*1000:.2f} ms\")\n",
    "print(f\"Sentences per second: {len(texts)/total_time:.2f}\")\n",
    "\n",
    "# Breakdown by component\n",
    "time_breakdown = {\n",
    "    'preprocessing': [],\n",
    "    'tokenization': [],\n",
    "    'pos_tagging': [],\n",
    "    'tone_processing': []\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    for component, time_val in result.processing_time.items():\n",
    "        time_breakdown[component].append(time_val)\n",
    "\n",
    "# Plot time breakdown\n",
    "import numpy as np\n",
    "\n",
    "avg_times = {k: np.mean(v)*1000 for k, v in time_breakdown.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "components = list(avg_times.keys())\n",
    "times = list(avg_times.values())\n",
    "\n",
    "plt.bar(components, times)\n",
    "plt.xlabel('Pipeline Component')\n",
    "plt.ylabel('Average Time (ms)')\n",
    "plt.title('Processing Time Breakdown')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add percentage labels\n",
    "total = sum(times)\n",
    "for i, (comp, t) in enumerate(zip(components, times)):\n",
    "    plt.text(i, t, f'{t/total*100:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}